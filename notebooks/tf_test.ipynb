{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5241143534584657102\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loads csv only, no images.\n",
    "'''\n",
    "def GetCSVs(sample_size):\n",
    "\n",
    "    # Name of folder\n",
    "    names = [\n",
    "        'Australia',\n",
    "        'China',\n",
    "        'Germany',\n",
    "        'NewarkLR',\n",
    "        'Switzerland',\n",
    "        'Amtrak',\n",
    "        'BostonMTBA',\n",
    "        'DenverRTD',\n",
    "        'LosAngelesMR',\n",
    "        'SeattleLLR',\n",
    "        'Netherlands'\n",
    "    ]\n",
    "\n",
    "    # Name of csv\n",
    "    abbr = [\n",
    "        'AUS',\n",
    "        'CHN',\n",
    "        'GRM',\n",
    "        'NEW',\n",
    "        'SWZ',\n",
    "        'AMT',\n",
    "        'BOS',\n",
    "        'DEN',\n",
    "        'LAA',\n",
    "        'SEA',\n",
    "        'NET'\n",
    "    ]\n",
    "    locations = dict(zip(names,abbr))\n",
    "\n",
    "    # Collect each csv into one df adding railway name\n",
    "    frames = []\n",
    "    for key,value in locations.items():\n",
    "        try:\n",
    "            filename = img_folder+key+'/'+value+'.csv'\n",
    "            tmp = pd.read_csv(filename,header=0)\n",
    "            tmp['Railway'] = key\n",
    "\n",
    "            # Take sample from each folder \n",
    "            tmp = tmp.sample(frac=sample_size).reset_index(drop=True)\n",
    "            frames.append(tmp)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    df = pd.concat(frames)\n",
    "\n",
    "    df = df.dropna()\n",
    "    df['Catenary'] = df['Catenary'].astype(int)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Open known non-catenary lines and add differntial to df\n",
    "    '''\n",
    "\n",
    "\n",
    "    zeros = df.Catenary.value_counts()[0]\n",
    "    ones = df.Catenary.value_counts()[1]\n",
    "\n",
    "    names = [\n",
    "        'Amtrak_non_cat_1',\n",
    "        'Amtrak_non_cat_2',\n",
    "        'Amtrak_non_cat_3'\n",
    "    ]\n",
    "\n",
    "    abbr = [\n",
    "        'ANC',\n",
    "        'ANC2',\n",
    "        'ANC3'\n",
    "    ]\n",
    "\n",
    "    locations['Amtrak_non_cat_1'] = 'ANC'\n",
    "    locations['Amtrak_non_cat_2'] = 'ANC2'\n",
    "    locations['Amtrak_non_cat_3'] = 'ANC3'\n",
    "\n",
    "    locations2 = dict(zip(names,abbr))\n",
    "\n",
    "    diff = ones - zeros\n",
    "\n",
    "    if diff > 0:\n",
    "        frames = []\n",
    "        for key,value in locations2.items():\n",
    "            try:\n",
    "                filename = img_folder+key+'/'+value+'.csv'\n",
    "                tmp = pd.read_csv(filename,header=0)\n",
    "                tmp['Railway'] = key\n",
    "                frames.append(tmp)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        try:\n",
    "            duds = pd.concat(frames)\n",
    "            duds = duds.dropna()\n",
    "            duds['Catenary'] = duds['Catenary'].astype(int) \n",
    "\n",
    "            duds = duds.sample(n=diff).reset_index(drop=True)\n",
    "            df = pd.concat([df,duds]).reset_index(drop=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            duds = duds.sample(len(duds.index.tolist())).reset_index(drop=True)\n",
    "            df = pd.concat([df,duds]).reset_index(drop=True)\n",
    "            \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Get image paths and labels as lists\n",
    "'''\n",
    "def GetPaths(df):\n",
    "\n",
    "    rows = df.index.tolist()\n",
    "    path = GetABSPath(img_folder)\n",
    "    img_paths = []\n",
    "    labels = []\n",
    "    for row in rows:\n",
    "        tmp = df.iloc[row]\n",
    "        img_path = path+'/'+tmp.Railway+'/'+tmp.Name+'.png'\n",
    "        img_paths.append(img_path)\n",
    "        label = int(tmp.Catenary)\n",
    "        labels.append(label)\n",
    "\n",
    "    print(len(img_paths))\n",
    "    \n",
    "    return img_paths,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetABSPath(folder):\n",
    "    return os.path.abspath(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessImage(img_path):\n",
    "    \n",
    "    img_raw = tf.io.read_file(img_path)\n",
    "    image = tf.io.decode_png(img_raw, channels=3)\n",
    "    image = tf.image.resize(image, [192,192])\n",
    "    print(image.shape)\n",
    "    image /= 255.0  # normalize to [0,1] range\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitDataSet(img_paths, labels):\n",
    "    \n",
    "    # split lists into training/test    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(img_paths,labels,test_size = .2, random_state=42)\n",
    "\n",
    "    # Read images/labels into tensor data    \n",
    "    train_path_ds = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "    train_image_ds = train_path_ds.map(PreprocessImage, num_parallel_calls=AUTOTUNE)\n",
    "    train_label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(y_train, tf.int64))\n",
    "    \n",
    "    # Combine into dataset     \n",
    "    train_image_label_ds = tf.data.Dataset.zip((train_image_ds, train_label_ds))\n",
    "    \n",
    "    \n",
    "    test_path_ds = tf.data.Dataset.from_tensor_slices(X_test)\n",
    "    test_image_ds = test_path_ds.map(PreprocessImage, num_parallel_calls=AUTOTUNE)\n",
    "    test_label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(y_test, tf.int64))\n",
    "    \n",
    "    test_image_label_ds = tf.data.Dataset.zip((test_image_ds, test_label_ds))\n",
    "    \n",
    "    return train_image_label_ds, test_image_label_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassifyImages(train_image_label_ds, test_image_label_ds):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Flatten(input_shape=(192,192,3)),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    steps_per_epoch = int(tf.ceil(len(img_paths)/batch_size).numpy())\n",
    "    \n",
    "    model.fit(train_image_label_ds,steps_per_epoch=steps_per_epoch, epochs=10)\n",
    "\n",
    "    model.evaluate(test_image_label_ds,steps=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Shuffle/batch/prefetch/Set Range\n",
    "'''\n",
    "def ShuffleBatch(ds_dict,buff,BATCH_SIZE = 32):\n",
    "    \n",
    "    ds = ds_dict.shuffle(buffer_size = buff)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches, in the background while the model is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    # ds\n",
    "\n",
    "    def change_range(image,label):\n",
    "        return 2*image-1, label\n",
    "\n",
    "    keras_ds = ds.map(change_range)\n",
    "    \n",
    "    return keras_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start program\n",
    "\n",
    "### Load from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_folder = '../data/output_images/'\n",
    "\n",
    "sample_size = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] File b'../data/output_images/China/CHN.csv' does not exist: b'../data/output_images/China/CHN.csv'\n",
      "1    163\n",
      "0    163\n",
      "Name: Catenary, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = GetCSVs(sample_size)\n",
    "print(df['Catenary'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n"
     ]
    }
   ],
   "source": [
    "img_paths,labels = GetPaths(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 192, 3)\n",
      "(192, 192, 3)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Split into train/test\n",
    "'''\n",
    "\n",
    "train_image_label_ds, test_image_label_ds = SplitDataSet(img_paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Shuffle & batch\n",
    "'''\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = ShuffleBatch(train_image_label_ds,len(img_paths),BATCH_SIZE = batch_size) \n",
    "test_ds = ShuffleBatch(test_image_label_ds,len(img_paths),BATCH_SIZE = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send ds to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jessedecker/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:642: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "mobile_net = tf.keras.applications.MobileNetV2(input_shape=(192, 192, 3), include_top=False)\n",
    "mobile_net.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  mobile_net,\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  tf.keras.layers.Dense(len(['0','1']), activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 6, 6, 1280)\n"
     ]
    }
   ],
   "source": [
    "# The dataset may take a few seconds to start, as it fills its shuffle buffer.\n",
    "image_batch, label_batch = next(iter(train_ds))\n",
    "\n",
    "feature_map_batch = mobile_net(image_batch)\n",
    "print(feature_map_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min logit: 0.06212671\n",
      "max logit: 0.9378733\n",
      "\n",
      "Shape: (32, 2)\n"
     ]
    }
   ],
   "source": [
    "logit_batch = model(image_batch).numpy()\n",
    "\n",
    "print(\"min logit:\", logit_batch.min())\n",
    "print(\"max logit:\", logit_batch.max())\n",
    "print()\n",
    "\n",
    "print(\"Shape:\", logit_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_192 (Model) (None, 6, 6, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 2562      \n",
      "=================================================================\n",
      "Total params: 2,260,546\n",
      "Trainable params: 2,562\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11/11 [==============================] - 18s 2s/step - loss: 0.8604 - acc: 0.4773\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.6973 - acc: 0.6023\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.6025 - acc: 0.6449\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.5234 - acc: 0.7472\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 16s 1s/step - loss: 0.4839 - acc: 0.7642\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = int(tf.ceil(len(img_paths)/batch_size).numpy())\n",
    "\n",
    "model.fit(train_ds, epochs=10, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "model.evaluate(test_ds,steps=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ClassifyImages(train_ds, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Image Walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filename and preview\n",
    "\n",
    "img_path = image_paths[0]\n",
    "plt.imshow(Image.open(img_path))\n",
    "print(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PreprocessImage(img_path[0])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(PreprocessImage(img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build tf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape: ', repr(path_ds.output_shapes))\n",
    "print('type: ', path_ds.output_types)\n",
    "print()\n",
    "print(path_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ds = path_ds.map(PreprocessImage, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "for n,image in enumerate(image_ds.take(4)):\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.imshow(image)\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(caption_image(all_image_paths[n]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(all_image_labels, tf.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
