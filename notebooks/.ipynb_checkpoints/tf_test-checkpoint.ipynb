{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WriteJSON(obj,filename):\n",
    "    with open(filename, 'w+') as outfile:\n",
    "        try:\n",
    "            obj_json = json.dumps(obj, sort_keys=True, indent=4,default=str)\n",
    "            outfile.write(obj_json)\n",
    "        except Exception as e:\n",
    "            print(e, file=sys.stderr)\n",
    "            print('File not written.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadJSON(filename):\n",
    "    obj = []\n",
    "    try: \n",
    "        with open(filename, 'r') as infile:\n",
    "            obj = json.load(infile)\n",
    "    except Exception as e:\n",
    "        print(e, file=sys.stderr)\n",
    "        print('File not found.')\n",
    "        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FitAndScoreCLA(features,labels,classifiers,testSize=0.20):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features,labels,test_size = testSize, random_state=42)\n",
    "    \n",
    "    clfs = []\n",
    "    for classifier in classifiers:\n",
    "        tmp = {}\n",
    "        clf = classifier['Method']\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Get report and matrix for display\n",
    "        print('Classification report for  -',classifier['Name'])\n",
    "        print('-----------------------------------------------------------------------------------------------')\n",
    "        print(\" %s:\\n%s\\n\"% (clf, classification_report(y_test, y_pred)))\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\n",
    "        print(classifier['Name'],'Confusion Matrix')\n",
    "        print('   P0 \\t P1 ')\n",
    "        print('A0',tn,'\\t',fp)\n",
    "        print('A1',fn,'\\t',tp)\n",
    "        print('\\n')\n",
    "        \n",
    "        print('r^2: ',r2)\n",
    "        \n",
    "        # Get report and matrix for file\n",
    "        clr = classification_report(y_test, y_pred,output_dict=True)\n",
    "        cnm = list(confusion_matrix(y_test,y_pred))\n",
    "        \n",
    "        tmp[classifier['Name']] = {'Report':clr,\n",
    "                                  'Matrix':cnm}\n",
    "        clfs.append(tmp)  \n",
    "        \n",
    "    # Open results file, append new result, write to file\n",
    "    resultsObj = ReadJSON(results_file)\n",
    "    \n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "    \n",
    "    currResults = {'Description':description,\n",
    "                   'classifiers':clfs,\n",
    "                   'Run Time':date_time,\n",
    "                   'Sample Size':sample_size,\n",
    "                   'Image Resolution':img_size,\n",
    "                   'Counts':{'0':dict(df.Catenary.value_counts())[0],'1':dict(df.Catenary.value_counts())[1]},\n",
    "              }\n",
    "    \n",
    "    resultsObj.append(currResults)\n",
    "    WriteJSON(resultsObj,results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Setup classifiers\n",
    "'''\n",
    "\n",
    "BGN = {'Name':'BGN',\n",
    "       'Method': GaussianNB()}\n",
    "\n",
    "DTC = {'Name':'DTC',\n",
    "       'Method': DecisionTreeClassifier()}\n",
    "\n",
    "KNN = {'Name':'KNN',\n",
    "       'Method': KNeighborsClassifier()}\n",
    "\n",
    "SVM = {'Name':'SVM',\n",
    "       'Method': SVC(gamma=0.001)}\n",
    "\n",
    "\n",
    "classifiers = [BGN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for  - BGN\n",
      "-----------------------------------------------------------------------------------------------\n",
      " GaussianNB(priors=None, var_smoothing=1e-09):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.17      0.22         6\n",
      "           1       0.44      0.67      0.53         6\n",
      "\n",
      "   micro avg       0.42      0.42      0.42        12\n",
      "   macro avg       0.39      0.42      0.38        12\n",
      "weighted avg       0.39      0.42      0.38        12\n",
      "\n",
      "\n",
      "BGN Confusion Matrix\n",
      "   P0 \t P1 \n",
      "A0 1 \t 5\n",
      "A1 2 \t 4\n",
      "\n",
      "\n",
      "r^2:  -1.3333333333333335\n",
      "File not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '../data/results/results3.json'\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Run Classifier\n",
    "'''\n",
    "\n",
    "FitAndScoreCLA(features,labels,classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parameters \n",
    "----------\n",
    "Set for each test. \n",
    "\n",
    "\n",
    "img_folder: Root folder of image collection\n",
    "\n",
    "results_file: JSON file for output of results and metadata\n",
    "\n",
    "description: String for labeling/notes\n",
    "\n",
    "sample_size: Sample size to pull from each csv, 0-1\n",
    "\n",
    "img_size: Native resolution is 1280x1280\n",
    "\n",
    "'''\n",
    "\n",
    "img_folder = '../data/output_images/'\n",
    "\n",
    "results_file = '../data/results/'+'results3.json'\n",
    "\n",
    "description = 'All ten image sets. Three quarters resolution.'\n",
    "\n",
    "sample_size = .5\n",
    "\n",
    "img_size = (640,640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File b'../data/output_images/China/CHN.csv' does not exist\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Catenary</th>\n",
       "      <th>Railway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>151.1934804_-33.8669032</td>\n",
       "      <td>151.193480</td>\n",
       "      <td>-33.866903</td>\n",
       "      <td>1</td>\n",
       "      <td>Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153.0118785_-27.5267477</td>\n",
       "      <td>153.011878</td>\n",
       "      <td>-27.526748</td>\n",
       "      <td>1</td>\n",
       "      <td>Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>151.1451506_-33.8837061</td>\n",
       "      <td>151.145151</td>\n",
       "      <td>-33.883706</td>\n",
       "      <td>1</td>\n",
       "      <td>Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144.99387769999998_-37.8302679</td>\n",
       "      <td>144.993878</td>\n",
       "      <td>-37.830268</td>\n",
       "      <td>1</td>\n",
       "      <td>Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145.1196571_-37.875116999999996</td>\n",
       "      <td>145.119657</td>\n",
       "      <td>-37.875117</td>\n",
       "      <td>1</td>\n",
       "      <td>Australia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Name   Longitude   Latitude  Catenary    Railway\n",
       "0          151.1934804_-33.8669032  151.193480 -33.866903         1  Australia\n",
       "1          153.0118785_-27.5267477  153.011878 -27.526748         1  Australia\n",
       "2          151.1451506_-33.8837061  151.145151 -33.883706         1  Australia\n",
       "3   144.99387769999998_-37.8302679  144.993878 -37.830268         1  Australia\n",
       "4  145.1196571_-37.875116999999996  145.119657 -37.875117         1  Australia"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Loads csv only, no images.\n",
    "'''\n",
    "\n",
    "# Name of folder\n",
    "names = [\n",
    "    'Australia',\n",
    "    'China',\n",
    "    'Germany',\n",
    "    'NewarkLR',\n",
    "    'Switzerland',\n",
    "    'Amtrak',\n",
    "    'BostonMTBA',\n",
    "    'DenverRTD',\n",
    "    'LosAngelesMR',\n",
    "    'SeattleLLR',\n",
    "    'Netherlands'\n",
    "]\n",
    "\n",
    "# Name of csv\n",
    "abbr = [\n",
    "    'AUS',\n",
    "    'CHN',\n",
    "    'GRM',\n",
    "    'NEW',\n",
    "    'SWZ',\n",
    "    'AMT',\n",
    "    'BOS',\n",
    "    'DEN',\n",
    "    'LAA',\n",
    "    'SEA',\n",
    "    'NET'\n",
    "]\n",
    "locations = dict(zip(names,abbr))\n",
    "\n",
    "# Collect each csv into one df adding railway name\n",
    "frames = []\n",
    "for key,value in locations.items():\n",
    "    try:\n",
    "        filename = img_folder+key+'/'+value+'.csv'\n",
    "        tmp = pd.read_csv(filename,header=0)\n",
    "        tmp['Railway'] = key\n",
    "        \n",
    "        # Take sample from each folder \n",
    "        tmp = tmp.sample(frac=sample_size).reset_index(drop=True)\n",
    "        frames.append(tmp)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "df = pd.concat(frames)\n",
    "\n",
    "df = df.dropna()\n",
    "df['Catenary'] = df['Catenary'].astype(int)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    160\n",
       "0     83\n",
       "Name: Catenary, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Catenary.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 5)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Open known non-catenary lines and add differntial to df\n",
    "'''\n",
    "\n",
    "zeros = df.Catenary.value_counts()[0]\n",
    "ones = df.Catenary.value_counts()[1]\n",
    "\n",
    "names = [\n",
    "    'Amtrak_non_cat_1',\n",
    "    'Amtrak_non_cat_2',\n",
    "    'Amtrak_non_cat_3'\n",
    "]\n",
    "\n",
    "abbr = [\n",
    "    'ANC',\n",
    "    'ANC2',\n",
    "    'ANC3'\n",
    "]\n",
    "locations = dict(zip(names,abbr))\n",
    "\n",
    "diff = ones - zeros\n",
    "\n",
    "if diff > 0:\n",
    "    frames = []\n",
    "    for key,value in locations.items():\n",
    "        try:\n",
    "            filename = img_folder+key+'/'+value+'.csv'\n",
    "            tmp = pd.read_csv(filename,header=0)\n",
    "            tmp['Railway'] = key\n",
    "            frames.append(tmp)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    try:\n",
    "        duds = pd.concat(frames)\n",
    "        duds = duds.dropna()\n",
    "        duds['Catenary'] = duds['Catenary'].astype(int) \n",
    "        \n",
    "        duds = duds.sample(n=diff).reset_index(drop=True)\n",
    "        df = pd.concat([df,duds]).reset_index(drop=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        duds = duds.sample(len(duds.index.tolist())).reset_index(drop=True)\n",
    "        df = pd.concat([df,duds]).reset_index(drop=True)\n",
    "        \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    160\n",
       "0    160\n",
       "Name: Catenary, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Catenary.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Catenary</th>\n",
       "      <th>Image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[[[91, 82, 71, 255], [86, 81, 72, 255], [87, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[[[122, 112, 100, 255], [130, 119, 108, 255], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[[[81, 79, 73, 255], [63, 61, 53, 255], [95, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[[[109, 106, 92, 255], [122, 121, 106, 255], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[[[57, 71, 49, 255], [62, 68, 50, 255], [86, 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Catenary                                              Image\n",
       "0         1  [[[91, 82, 71, 255], [86, 81, 72, 255], [87, 8...\n",
       "1         1  [[[122, 112, 100, 255], [130, 119, 108, 255], ...\n",
       "2         1  [[[81, 79, 73, 255], [63, 61, 53, 255], [95, 9...\n",
       "3         1  [[[109, 106, 92, 255], [122, 121, 106, 255], [...\n",
       "4         1  [[[57, 71, 49, 255], [62, 68, 50, 255], [86, 8..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Load images into df\n",
    "'''\n",
    "rows = df.index.tolist()\n",
    "\n",
    "images = []\n",
    "for row in rows:\n",
    "    img_path = img_folder+df.iloc[row]['Railway']+'/'+df.iloc[row]['Name']+'.png'\n",
    "    img = Image.open(img_path).convert('RGBA')\n",
    "    img.thumbnail(img_size, Image.ANTIALIAS)\n",
    "    data = np.asarray(img)\n",
    "#     data = data.flatten()\n",
    "    # Append img instead of data if you want as image       \n",
    "    images.append(data)\n",
    "    \n",
    "df['Image'] = images\n",
    "\n",
    "cols = ['Catenary','Image']\n",
    "df = df[cols]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading images into tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.asarray(df.Catenary.tolist())\n",
    "features = np.asarray(df.Image.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.reshape(len(features),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 320, 320, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features,labels,test_size = 0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "#   tf.keras.layers.Flatten(input_shape=(26, 26)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "48/48 [==============================] - 23s 489ms/step - loss: 4.6582 - acc: 0.1458\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 5s 112ms/step - loss: 6.7637 - acc: 0.5625\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 5s 107ms/step - loss: 7.6245 - acc: 0.5208\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 5s 108ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 5s 107ms/step - loss: 8.0590 - acc: 0.5000\n",
      "12/12 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.05904769897461, 0.5]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.2201 - acc: 0.9344\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0964 - acc: 0.9707\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.0691 - acc: 0.9783\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0530 - acc: 0.9830\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0432 - acc: 0.9861\n",
      "10000/10000 [==============================] - 0s 31us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07107611664508004, 0.979]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "(10,)\n",
      "(tf.float32, tf.int32)\n",
      "(TensorShape([]), TensorShape([Dimension(100)]))\n",
      "(tf.float32, (tf.float32, tf.int32))\n",
      "(TensorShape([Dimension(10)]), (TensorShape([]), TensorShape([Dimension(100)])))\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
    "print(dataset1.output_types)  # ==> \"tf.float32\"\n",
    "print(dataset1.output_shapes)  # ==> \"(10,)\"\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   (tf.random_uniform([4]),\n",
    "    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
    "print(dataset2.output_types)  # ==> \"(tf.float32, tf.int32)\"\n",
    "print(dataset2.output_shapes)  # ==> \"((), (100,))\"\n",
    "\n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "print(dataset3.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))\n",
    "print(dataset3.output_shapes)  # ==> \"(10, ((), (100,)))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': tf.float32, 'b': tf.int32}\n",
      "{'a': TensorShape([]), 'b': TensorShape([Dimension(100)])}\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "   {\"a\": tf.random_uniform([4]),\n",
    "    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})\n",
    "print(dataset.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\n",
    "print(dataset.output_shapes)  # ==> \"{'a': (), 'b': (100,)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-04c63af7ff4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "for i in range(100):\n",
    "  value = sess.run(next_element)\n",
    "  assert i == value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
